---
layout: post
title:  "Week 3 - Linear Regression and EDA
date:   2015-11-15 12:00:00
categories: feature
tags: featured datascience galvanize bootcamp
image:
---

#Week 3 - Linear Regression and EDA


This week was a flash introduction to the topic of regression. The previous weeks were spent teaching us how to get data on our computer in a workable form using Python. This week we began learning how to use that data to predict future results. As data scientists, or business people in general, we are interested in looking at past data to make strategic decisions about future courses of action. This could be predicting the conversion rate of a website, the profitability of a stock trading strategy, or the effectiveness of a experimental drug. Regression, in it’s multiple forms, is one of the primary techniques used in this effort.

As some of you might know, the last weeks of the Galvanize program are spent on each student’s individual Capstone Project. This is a large data science project used to illustrate to others, especially potential employers, the skill and level of work of which each student is now capable. The two topics on my short list are finance and sports. I do a lot of options trading and would be interested in analyzing historical options and stock price data to predict future results. I also love sports and would like to do some analysis on NBA or NFL data to predict game outcomes and player performances. I’ll likely do my project on something finance related as that can show potential employers I know how to turn data into profitable business insights. But then I also plan to do a few shorter sports projects that could become future blog posts.

On the personal side, I had a good week. I was little tired on Monday, but I made sure to get enough sleep the rest of the week. I’m well adjusted to the schedule now and know what to expect each day. I made sure to have the reading done ahead of each day’s class, which helped provide context during the lectures. I also began watching some extra [youtube videos](https://youtu.be/KsVBBJRb9TE?list=PLvxOuBpazmsND0vmkP1ECjTloiVz-pXla) on linear regression to reinforce the concepts. I also went to the gym in the morning for the first time on Thursday. The tough part is getting there, but then I felt better than average for the rest of the day. My goal for next week is to get in 2-3 morning gym sessions.

Overall, I’m learning a lot and enjoying the experience. I’m looking forward to how much more we’re going to learn in the coming weeks.


**Day 11: Linear Algebra and Exploratory Data Analysis**  
I felt like last week went pretty fast, but it felt like this weekend went even faster. I got off Friday evening and then what seemed like a few moments later, it was Sunday night and I was getting ready for Monday. After a few yawns, we started digging into some Linear Algebra (LA) review. I haven’t taken an LA class in about 10 years, so it’s taking a serious refresher to get put to speed. In data science, you are usually dealing with very large datasets assembled together as Matrices. A large part of LA is handling and manipulating matrices. The morning lecture and sprint covered basic Matrix operations and the corresponding Numpy functions to handle them. The afternoon was an intro to [Exploratory Data Analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis) (EDA). EDA is a core piece of the data science workflow. When starting work on a new dataset, the first step is to clean the data. This entails removing/replacing missing values, ensuring the features in which you are interested are represented and a few other steps we haven’t covered. Once you have ‘clean’ data, now we get into EDA. Many times you have an idea of what you’re looking for in some data and other times you are just exploring to see if you can uncover some patterns. EDA involves playing around with the data, finding maximums and minimums, sorting different columns or rows, adding new columns, etc. However, the most efficient way of exploring data is to make some visualizations. [Humans process and interpret visual information faster and more accurately than written or verbal modalities](https://web.fe.up.pt/~tavares/downloads/publications/artigos/IJI_Manuscript_DA_JT.pdf). This makes creating several plots of the data an effective way to view the info. Today we used scatter plots, scatter matrices, box plots and bar charts. Most of the students are kind of clunky and slow when plotting now, but I feel we’ll all be faster as we get more practice.

**Day12: Linear Regression**  
Yesterday, we did a short linear regression (LR) example at the end of the EDA portion. Today we did full LR analysis in both the morning and the afternoon. LR is used to create projected values for each point in your dataset. It does this by fitting a line to the data and working to minimize the error, which is the difference between the projected and actual values. As a data scientist, we are often asked to create models to predict certain data in the future. We do this by creating a model that can predict historical data. Creating this prediction line noted above is one of the more simple model prediction techniques.

In today’s exercises, we used the [OLS](http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLS.html) (Ordinary Least Squares) regression module from the python library [Statsmodels](http://statsmodels.sourceforge.net/devel/index.html). After loading your data into a Pandas dataframe, it’s pretty easy to input your variables into the OLS function. The output can be used to create plots of the data and errors. In this exercise, we were trying to predict a person’s credit card balance based on a number of inputs, including income, credit rating, credit limit, age, race, number of credit cards and education level. After creating a model using all the available factors noted previously, we played around with removing and adding back different ones, then reviewing the accuracy of the output. We played around with several options, but eventually found a model containing only income, credit rating, age and student status was the best at predicting balance by minimizing errors and reducing collinearity. You can read more about collinearity [here](https://en.wikipedia.org/wiki/Multicollinearity).





**Day 13: Regularized Regression**  
Whereas the last couple days were all about Linear Regression, today we delved into different types of Regularized Regression (RR). LR set the tone for the basics of regression in general, which made it much easier to understand new regression techniques like Ridge and Lasso. LR results fall somewhere along the Bias and Variance spectrum. Because the prediction line is only a line, data that isn’t also linear, can be difficult to predict with LR. Therefore, using LR forces us to choose between a certain amount of Bias and a certain amount of Variance; and they are usually inversely correlated. See the image below for a reference point.

To combat this Bias/Variance tradeoff, RR first [normalizes](https://en.wikipedia.org/wiki/Multicollinearity) all the test observations so they have mean zero and variance of one. This allows us to deal with the error terms only. LR fits the regression line by minimizing the average error terms over all the training observations. [Ridge](https://en.wikipedia.org/wiki/Tikhonov_regularization) and [Lasso](https://en.wikipedia.org/wiki/Least_squares#Lasso_method) add an extra penalty term to the minimizing statistic to help with the Bias/Var issue. In the afternoon exercises, we built LR, Ridge and Lasso models for the same dataset and then compared the results to select the best one.


**Day 14: Assessment 3 and Logistic Regression**  
The day began with a two hour long assessment on all the material we’ve studied to date. It consisted of 10 questions on math and stats, 9 questions on python/numpy/pandas and 1 question on SQL. On the first assessment, I spent too long solving small issues on the python problems and, as a result, didn’t even get to start the math section. Therefore, I did the math section first today. I breezed through all but one of the problems as math and stats are still pretty fresh in my mind after preparing for the admissions technical interviews. The first half revolved around probability using Bayes’ Rule. The second half was one larger stats problem. It started by computing the sample mean and variance, and then moved into [t-tests](https://en.wikipedia.org/wiki/Student%27s_t-test) to evaluate some hypothesis tests. I felt good about this section.

Next, was the coding section. At a high level, the problems covered writing functions involving the following topics: coin flipping simulation, Bayesian A/B testing, t-tests, grouping and sorting the means of a Pandas dataframe, sorting and manipulating Numpy arrays and matrices, linear regression and the one SQL problem. I was able to finish all of the problems, but could only get eight of the ten correct using the test file.

Overall, I was very happy with my performance on the assessment. Since I don't have a background in coding or math/stats, I’m usually slower than some of my classmates that have these backgrounds. I always understand the material, but might have to lookup several things online (google, python/numpy/pandas/scipy/sklearn documentation, [stack overflow](http://stackoverflow.com/)) to finish the assignment. Today, I was able to finish all the problems in time and am sure I got most of them correct.

Because the assessment was so long, we didn’t have a solo exercise. The lectures and afternoon pair problems focused on [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression). Logistic regression is used when you want to predict an outcome that has a binary response. For example, one of our problems today was to predict if a given applicant is admitted to a graduate school based upon his or her undergraduate GPA, GRE score and a subjective prestige ranking of their undergraduate college. The student being admitted has a binary outcome; they either are admitted or denied. This is where logistic regression comes in handy. I admittedly, only understand a little about this type of regression right now, but I’m sure it’ll start to become more clear as we get further practice.

**Day 15: Gradient Descent**  
[Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) is a technique used for finding the local minimum of a function. It works by calculating the greatest slope in any direction for each variable, moving a given amount along the function in that direction, and repeating the process until any movement makes the slope calculation worse. In data science, we often have a function we estimate from a model and would like to find the minimum error or some other statistic requiring a minimum. Conversely, Gradient Ascent is the process of finding a maximum of a function by moving upwards along the function. This technique falls prey to the local minimum problem. If there are two minimums in a function and gradient descent is started near the lesser minimum, it will find the lesser minimum and stop. The algorithm can only see the slopes right near the current point of evaluation, and therefore isn’t able to tell if the minimum it has found is the global minimum of that function.

In today’s sprints, we spent a good amount of time programming logistic regression functions from scratch and running them on some new data. One of the more difficult parts of [binary classifiers](https://en.wikipedia.org/wiki/Binary_classification), of which logistic regression is one of them, is interpreting the results. The stats of false positives and false negatives, can be tough to keep straight in your head. We then evaluate the strength of our classifiers, by calculating various stats based on the accuracy of the predictions.