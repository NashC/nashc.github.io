---
layout: post
title:  "Week 9 - Case Study and Christmas"
date:   2015-12-27 12:00:00
categories: feature
tags: featured datascience galvanize bootcamp
image:
---

#Week 9 - Case Studies & Christmas

This is a shortened week due to Christmas being on Friday. We’re spending Monday and Tuesday doing a Fraud Detection case study. Wednesday will consist of the final assessment covering all the material of the class to date, and a short meeting to discuss our capstone projects.


**Days 38/39: Fraud Detection Case Study**  
I found the work of these two days very valuable and interesting. Throughout the course we’ve done all the different types of data science work, but here we put it together into a single end to end case study. All of our exercises to date have had step by step instructions of each task needed to complete the assignment. However, today was much more vague. We paired up and were given transaction data from what appeared to be a ticketing company such as Stubhub or Ticketmaster. We were tasked with building a model to detect fraudulent transactions, and building a web app which pulls new transactions from an online server and classifies them as fraud or not.

First step is EDA (exploratory data analysis) to see what we’re working with. Note, this was a supervised learning task as we had past transactions that had already been classified as fraudulent or not. We built several visualizations in iPython Notebook such as scatter plots, scatter matrices, bar plots and histograms. We saw several things that appeared common or interesting about the fraudulent transactions. For one specific example, we noticed several of the frauds had many of the data fields blank, zero, or NaN. For feature engineering, we created another column called ‘Falsy’ which counted the number of these types of empty data fields for a given transaction. This feature turned out to be statistically significant in all of our test models.

After EDA, we wanted to build a basic model simply to see where we were at. For this we used a [sklearn’s Random Forest] (http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html ) to get our feature importances. We also built a logistic regression model using [Statsmodels Logistic Regression] (http://statsmodels.sourceforge.net/0.6.0/generated/statsmodels.discrete.discrete_model.Logit.html ) as it provides a nice summary output of your model’s statistics such as R-Squared, and the coefficients and p-values of each of your variables. We had to eliminate several of our variables as they were not compatible with logistic regression. Our first model had 98% accuracy, which we were a little surprised by. However, our recall and precision were not very good.

In any data science model, you need to know what you’re optimizing for. In this case we wanted to optimize for [Recall] (https://en.wikipedia.org/wiki/Precision_and_recall). A high recall score means you are getting a very low rate of false negatives. In this fraud case study, we must consider the [cost benefit matrix] (https://en.wikipedia.org/wiki/Cost%E2%80%93benefit_analysis) associated with our [confusion matrix] (https://en.wikipedia.org/wiki/Confusion_matrix). The best way to figure out which metrics to optimize is to create a profit curve; however, for this exercise we did not have the exact cost benefit matrix for the company. Our assumption was that a false negative, or the company mistakenly classifying a fraudulent transaction as non-fraudulent, would be more costly than a false positive, or mistakenly classifying a non-fraudulent transaction as a fraudulent one. If we let a fraud happen (false negative) we risk losing the amount of that entire transaction as we’ll likely have to refund the customer. If we flag a non-fraud, the company will likely do some follow up work investigating the transaction. This investigation costs money for labor and could lead to churn through  
customer dissatisfaction or them canceling because the transaction took too long. All these factors must be considered when optimizing a model based on domain knowledge. To circle back, we will be optimizing for Recall as this leads to the lowest amount of false negative, which we deemed to be the most costly type of classification.

Our first model had great accuracy, but we needed to improve the recall. How could we do this? After speaking with some other teams, we noticed several of them were SMOTE’ing the data. Since only 6.5% of our training transactions were fraud, it’s a small sample for our model to train itself to predict fraud. What can we do to help this? [SMOTE (Synthetic Minority Over-Sampling Technique)] (https://www.jair.org/media/953/live-953-2037-jair.pdf) is a technique of randomly over sampling the minority class, in this case our small number of fraud transactions, so the training data represents some larger target proportion of the training data. We tried a few different target levels, but found the default of 50% fraud vs no fraud gave us the best recall score. SMOTE’ing can come at a cost as you trade recall at the cost of precision. I encourage you to read more on this topic if you are interested.

After SMOTE’ing the data, we were getting good recall, but bad accuracy. Since we were building a binary classification model, we wanted to regularize our data. We used [sklearn’s StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) preprocessing module for this purpose. After this we had our accuracy around 90% and our recall around 95%.

At this point, we wanted to try out different models to see if we could improve our scores. We tried [sklearn’s Logistic Classifier] (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and [Support Vector Classification] (http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) models. We also tried a [Grid Search] (http://scikit-learn.org/stable/modules/grid_search.html), but it didn’t finish after an hour, so we abandoned that. Instead, we manually tried a few options on each model and recorded our metrics for each version. We found the original Statsmodels Logit model with the fit Method=‘powell’ to be our best performer.

We reached this point at about lunchtime on day two. We likely spent too much time in EDA and preprocessing on day one, but at this point in our data science learning journey, I think it’s understandable. Now we’ve got the afternoon to build a web app that can connect to an online server to receive data on what would be new transactions and then classify them as fraud or not.

The hardest parts here were properly connecting to the remote server, authenticating our IP address and converting the received data into an interpretable input for our model. After a couple hours, we got all those parts working. Next we had to decide how to present the outputs of our model. We only had 45 minutes, so we simply presented the fraud level of the most recently received transaction from the remote server. We used a few [Bootstrap] (http://getbootstrap.com/) assets for the site, but otherwise it was very basic. Other teams who had more time for the web app portion built dashboards showing some metrics no the aggregate history of all new transactions received from the server.

What did I learn after spending two days on a single data science project? I learned I need to do more of these. Experiencing the whole process from start to finish without much direction, is very different from doing a small part of the process with step by step instructions. One of the instructors said today that most students learn more during the capstone project than they did during the whole class up until that point. I wouldn’t put it exactly like that, but I understand what he meant. Going through an entire project puts the big picture into perspective. After this experience, I’m eager to get working on the capstone project, I just need to solidify what that project will be.

**Day 40: Final Assessment**  
Short post today. We did the final assessment which was broke into two parts. The first segment was 2 hours 15 minutes and covered all the material from the entire class. My time management was pretty good, but I did have to leave two questions blank as I didn’t know the answer off hand and also didn’t have time to research the answer. I was feeling pretty good after part one.

A 15 minute break and we were into part two. This was a one hour segment where we were given some news articles and asked to build a classifier model to tell predict the topic of the article based on its text. I got a good start stripping the text of stop words and punctuation, lemmatizing the words, extracting the vocabulary of the corpus and creating a sparse vector matrix for each article. I then wanted to plug these vectors into a classifier to predict the topic, but I had run out of time at this point. I turned in what I had, but put a note to schedule a 1 on 1 with one of the instructors to walk me through the answer when we return from holiday break.

**Weekly Summary**  
We’re done with the structured part of the curriculum. No more lectures or sprints. We have a week off for holidays then back for two weeks on our projects. I found I’m not likely to get the data for my Crunchbase project, so I’m brainstorming alternatives. I’m looking forward to some mental relaxation over the break, but am also excited to get right back at it.